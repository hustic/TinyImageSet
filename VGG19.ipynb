{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pycm in /opt/firedrake/lib/python3.6/site-packages (2.6)\n",
      "Requirement already satisfied: livelossplot in /opt/firedrake/lib/python3.6/site-packages (0.5.0)\n",
      "Requirement already satisfied: numpy>=1.9.0 in /opt/firedrake/lib/python3.6/site-packages (from pycm) (1.18.2)\n",
      "Requirement already satisfied: art>=1.8 in /opt/firedrake/lib/python3.6/site-packages (from pycm) (4.5)\n",
      "Requirement already satisfied: matplotlib; python_version >= \"3.6\" in /opt/firedrake/lib/python3.6/site-packages (from livelossplot) (3.2.1)\n",
      "Requirement already satisfied: ipython in /opt/firedrake/lib/python3.6/site-packages (from livelossplot) (7.13.0)\n",
      "Requirement already satisfied: coverage>=4.1 in /opt/firedrake/lib/python3.6/site-packages (from art>=1.8->pycm) (4.4)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/firedrake/lib/python3.6/site-packages (from matplotlib; python_version >= \"3.6\"->livelossplot) (2.8.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/firedrake/lib/python3.6/site-packages (from matplotlib; python_version >= \"3.6\"->livelossplot) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/firedrake/lib/python3.6/site-packages (from matplotlib; python_version >= \"3.6\"->livelossplot) (1.1.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/firedrake/lib/python3.6/site-packages (from matplotlib; python_version >= \"3.6\"->livelossplot) (2.4.6)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /opt/firedrake/lib/python3.6/site-packages (from ipython->livelossplot) (4.8.0)\n",
      "Requirement already satisfied: pickleshare in /opt/firedrake/lib/python3.6/site-packages (from ipython->livelossplot) (0.7.5)\n",
      "Requirement already satisfied: setuptools>=18.5 in /opt/firedrake/lib/python3.6/site-packages (from ipython->livelossplot) (46.1.3)\n",
      "Requirement already satisfied: jedi>=0.10 in /opt/firedrake/lib/python3.6/site-packages (from ipython->livelossplot) (0.16.0)\n",
      "Requirement already satisfied: backcall in /opt/firedrake/lib/python3.6/site-packages (from ipython->livelossplot) (0.1.0)\n",
      "Requirement already satisfied: decorator in /opt/firedrake/lib/python3.6/site-packages (from ipython->livelossplot) (4.4.2)\n",
      "Requirement already satisfied: pygments in /opt/firedrake/lib/python3.6/site-packages (from ipython->livelossplot) (2.6.1)\n",
      "Requirement already satisfied: traitlets>=4.2 in /opt/firedrake/lib/python3.6/site-packages (from ipython->livelossplot) (4.3.3)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/firedrake/lib/python3.6/site-packages (from ipython->livelossplot) (3.0.5)\n",
      "Requirement already satisfied: six>=1.5 in /opt/firedrake/lib/python3.6/site-packages (from python-dateutil>=2.1->matplotlib; python_version >= \"3.6\"->livelossplot) (1.14.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/firedrake/lib/python3.6/site-packages (from pexpect; sys_platform != \"win32\"->ipython->livelossplot) (0.6.0)\n",
      "Requirement already satisfied: parso>=0.5.2 in /opt/firedrake/lib/python3.6/site-packages (from jedi>=0.10->ipython->livelossplot) (0.6.2)\n",
      "Requirement already satisfied: ipython-genutils in /opt/firedrake/lib/python3.6/site-packages (from traitlets>=4.2->ipython->livelossplot) (0.2.0)\n",
      "Requirement already satisfied: wcwidth in /opt/firedrake/lib/python3.6/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->livelossplot) (0.1.9)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/opt/firedrake/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "!pip install pycm livelossplot\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "from livelossplot import PlotLosses\n",
    "from pycm import *\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda installed! Running on GPU!\n"
     ]
    }
   ],
   "source": [
    "def set_seed(seed):\n",
    "    \"\"\"\n",
    "    Use this to set ALL the random seeds to a fixed value and take out any randomness from cuda kernels\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    torch.backends.cudnn.benchmark = False  ##uses the inbuilt cudnn auto-tuner to find the fastest convolution algorithms. -\n",
    "    torch.backends.cudnn.enabled   = False\n",
    "\n",
    "    return True\n",
    "\n",
    "device = 'cpu'\n",
    "if torch.cuda.device_count() > 0 and torch.cuda.is_available():\n",
    "    print(\"Cuda installed! Running on GPU!\")\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    print(\"No GPU available!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "100000\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json \n",
    "\n",
    "\n",
    "with open(\"mapping.json\",'r', encoding='UTF-8') as f:\n",
    "     load_dict = json.load(f)\n",
    "\n",
    "\n",
    "train_file = glob.glob('train/*/images/*.JPEG')\n",
    "print(len(train_file))\n",
    "train_data = []\n",
    "labels = []\n",
    "count = 0\n",
    "\n",
    "\n",
    "for f in train_file:\n",
    "    img = np.array(Image.open(f))\n",
    "    label_name = f.split('/')[1]\n",
    "    label = load_dict.get(label_name)\n",
    "    if img.shape != (64,64,3):\n",
    "        img = np.stack((img,)*3, axis=-1)\n",
    "    \n",
    "    train_data.append(img)\n",
    "    labels.append(label)\n",
    "    count += 1\n",
    "\n",
    "print(len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 64, 64, 3)\n",
      "(100000,)\n"
     ]
    }
   ],
   "source": [
    "train_data = np.array(train_data)\n",
    "labels = np.array(labels)\n",
    "print(train_data.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.48024579 0.44807218 0.39754775]\n",
      "[0.27698641 0.26906449 0.28208191]\n"
     ]
    }
   ],
   "source": [
    "mean = train_data.mean(axis=(0,1,2))/255\n",
    "std = train_data.std(axis=(0,1,2))/255\n",
    "\n",
    "print(mean)\n",
    "print(std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "shuffler = StratifiedShuffleSplit(n_splits=1, test_size=0.1, random_state=42).split(train_data, labels)\n",
    "\n",
    "indices = [(train_idx, validation_idx) for train_idx, validation_idx in shuffler][0]\n",
    "#valid_idx = list(set(train_idx).difference(set(train_dataset)))\n",
    "\n",
    "print(len(indices[0]))\n",
    "print(len(indices[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([90000, 3, 64, 64])\n",
      "torch.Size([10000, 3, 64, 64])\n",
      "torch.Size([90000])\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = torch.tensor(train_data[indices[0]]).float(), torch.tensor(labels)[indices[0]]\n",
    "X_val, y_val = torch.tensor(train_data[indices[1]]).float(), torch.tensor(labels)[indices[1]]\n",
    "\n",
    "\n",
    "X_train = X_train.permute(0,3,1,2)\n",
    "X_val = X_val.permute(0,3,1,2)\n",
    "\n",
    "\n",
    "print(X_train.size())\n",
    "print(X_val.size())\n",
    "print(y_train.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset \n",
    "\n",
    "class CustomImageTensorDataset(Dataset):\n",
    "    def __init__(self, data, targets, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data (Tensor): A tensor containing the data e.g. images\n",
    "            targets (Tensor): A tensor containing all the labels\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "        self.transform = transform\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample, label = self.data[idx], self.targets[idx]\n",
    "        #sample = sample.view(3, 64, 63).float()/255.\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "\n",
    "        return sample, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = torchvision.transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((256,256)),\n",
    "    transforms.RandomCrop(224, padding=4),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)])\n",
    "\n",
    "valid_transform = torchvision.transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)])\n",
    "\n",
    "train_set = CustomImageTensorDataset(X_train, y_train.long(), transform=train_transform)\n",
    "valid_set = CustomImageTensorDataset(X_val, y_val.long(), transform=valid_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters\n",
    "\n",
    "seed = 42\n",
    "lr = 1e-2\n",
    "momentum = 0.9\n",
    "batch_size = 32\n",
    "test_batch_size = 1000\n",
    "n_epochs = 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "valid_loader = DataLoader(valid_set, batch_size=test_batch_size, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, data_loader):\n",
    "    model.train() ## the model is in the training mode so the parameters(weights)to be optimised will be updatad at each step\n",
    "    train_loss, train_accuracy = 0, 0\n",
    "    for X, y in data_loader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        a2 = model(X.view(-1, 3, 224, 224))\n",
    "        loss = criterion(a2, y)\n",
    "        loss.backward()\n",
    "        train_loss += loss*X.size(0)\n",
    "        y_pred = F.log_softmax(a2, dim=1).max(1)[1]\n",
    "        train_accuracy += accuracy_score(y.cpu().numpy(), y_pred.detach().cpu().numpy())*X.size(0) ## .cpu cpoies the tensor to the cpu to evaluate the accuracy score since it is a scikitlearn metric which does not run on GPUs\n",
    "        optimizer.step()  \n",
    "        \n",
    "    return train_loss/len(data_loader.dataset), train_accuracy/len(data_loader.dataset)  ## loss and accuracy calculated give the total for all batches and must be divided by the number of batch size to give the average values.\n",
    "\n",
    "def validate(model, criterion, data_loader):\n",
    "    model.eval() ## model is set to evaluation mode to freeze the parameters and ensure weights are not updated at each step so that the trained model is used to evaluate the validation loss\n",
    "    validation_loss, validation_accuracy = 0., 0.\n",
    "    for X, y in data_loader:\n",
    "        with torch.no_grad():\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            a2 = model(X.view(-1, 3, 224, 224))\n",
    "            loss = criterion(a2, y)\n",
    "            validation_loss += loss*X.size(0)\n",
    "            y_pred = F.log_softmax(a2, dim=1).max(1)[1]\n",
    "            validation_accuracy += accuracy_score(y.cpu().numpy(), y_pred.cpu().numpy())*X.size(0)\n",
    "            \n",
    "    return validation_loss/len(data_loader.dataset), validation_accuracy/len(data_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "    ys, y_preds = [], []\n",
    "    for X, y in data_loader:\n",
    "        with torch.no_grad():\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            a2 = model(X.view(-1, 3, 64, 64))\n",
    "            #a2 = model(X.view(-1, 28*28)) #What does this have to look like for our conv-net? Make the changes!\n",
    "            y_pred = F.log_softmax(a2, dim=1).max(1)[1]\n",
    "            ys.append(y.cpu().numpy())\n",
    "            y_preds.append(y_pred.cpu().numpy())\n",
    "            \n",
    "    return np.concatenate(y_preds, 0),  np.concatenate(ys, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    " \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import copy\n",
    "import os\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, lr=1e-2, num_epochs=30):\n",
    " \n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    valid_loader = DataLoader(valid_set, batch_size=test_batch_size, shuffle=False, num_workers=0)\n",
    "  \n",
    "    liveloss = PlotLosses()\n",
    "    for epoch in range(num_epochs):\n",
    "        if epoch == 60 or epoch == 75:\n",
    "            lr *= 0.1\n",
    "            optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
    "        \n",
    "        logs = {}\n",
    "        train_loss, train_accuracy = train(model, optimizer, criterion, train_loader)\n",
    "\n",
    "        logs['' + 'log loss'] = train_loss.item()\n",
    "        logs['' + 'accuracy'] = train_accuracy.item()\n",
    "\n",
    "        validation_loss, validation_accuracy = validate(model, criterion, valid_loader)\n",
    "        logs['val_' + 'log loss'] = validation_loss.item()\n",
    "        logs['val_' + 'accuracy'] = validation_accuracy.item()\n",
    "\n",
    "        liveloss.update(logs)\n",
    "        liveloss.draw()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG19(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG19, self).__init__()\n",
    "        # 224x224\n",
    "        self.c1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.c2 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.s3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # 112x112\n",
    "        self.c4 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.c5 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.s6 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # 56x56\n",
    "        self.c7 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.c8 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.s9 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # 28x28\n",
    "        self.c10 = nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1)\n",
    "        self.c11 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1)\n",
    "        self.s12 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # 14x14\n",
    "        self.c13 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1)\n",
    "        self.c14 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1)\n",
    "        self.s15 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # 7x7\n",
    "        self.s16 = nn.AvgPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        # 4x4\n",
    "        self.f17 = nn.Linear(8192, 2048)\n",
    "        self.output = nn.Linear(2048, 200)\n",
    "        self.act = nn.PReLU()\n",
    "        self.bn = nn.BatchNorm1d(8192)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.act(self.c1(x))\n",
    "        x = self.act(self.c2(x))\n",
    "        x = self.act(self.s3(x))\n",
    "        x = self.act(self.c4(x))\n",
    "        x = self.act(self.c5(x))\n",
    "        x = self.act(self.s6(x))\n",
    "        x = self.act(self.c7(x))\n",
    "        x = self.act(self.c8(x))\n",
    "        x = self.act(self.s9(x))\n",
    "        x = self.act(self.c10(x))\n",
    "        x = self.act(self.c11(x))\n",
    "        x = self.act(self.s12(x))\n",
    "        x = self.act(self.c13(x))\n",
    "        x = self.act(self.c14(x))\n",
    "        x = self.act(self.s15(x))\n",
    "        x = self.act(self.s16(x))\n",
    "        x = x.view(-1, 8192)  ##returns a \"flattened\" view of the 2d tensor as inputs for the fully connected layer\n",
    "        x = self.bn(x)\n",
    "        x = self.act(self.f17(x))\n",
    "        x = self.dropout(x)\n",
    "        return self.output(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-73598fb9de5e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVGG19\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Xavier Initialization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mweight_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/firedrake/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    423\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_backward_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/firedrake/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/firedrake/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    221\u001b[0m                 \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m                     \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/firedrake/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory"
     ]
    }
   ],
   "source": [
    "model = VGG19().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Xavier Initialization\n",
    "def weight_init(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight, gain=nn.init.calculate_gain('relu'))\n",
    "        nn.init.zeros_(m.bias)\n",
    "\n",
    "model.apply(weight_init)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2, momentum=0.9, weight_decay=5e-4)\n",
    "model = nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_model(model, criterion, optimizer, num_epochs=n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path='test/'\n",
    "\n",
    "test_transform = torchvision.transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean, std)])\n",
    "\n",
    "test_dataset = torchvision.datasets.ImageFolder(\n",
    "        root=data_path,\n",
    "        transform=test_transform\n",
    "    )\n",
    "\n",
    "import glob\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json \n",
    "\n",
    " \n",
    "\n",
    "test_file = glob.glob('test/images/*.JPEG')\n",
    "print(len(test_file))\n",
    "test_data = []\n",
    "test_names = []\n",
    "\n",
    " \n",
    "\n",
    "for f in test_file:\n",
    "    img = np.array(Image.open(f))\n",
    "    test_name = f.split('/')[2][5:-5]\n",
    "    #label = load_dict.get(label_name)\n",
    "    if img.shape != (64,64,3):\n",
    "        img = np.stack((img,)*3, axis=-1)\n",
    "    \n",
    "    test_data.append(img)\n",
    "    test_names.append(int(test_name))\n",
    " \n",
    "#y_test is only used for generating dataset. it's not the real labels\n",
    "y_test = np.array(test_names)\n",
    "y_test = torch.tensor(y_test)\n",
    " \n",
    "X_test = np.array(test_data)\n",
    "X_test = torch.tensor(X_test).float()\n",
    "X_test = X_test.permute(0,3,1,2)\n",
    " \n",
    "test_set = CustomImageTensorDataset(X_test, y_test.long(), transform=valid_transform)\n",
    "\n",
    "# test_data = torch.Tensor(test_data)\n",
    "# labels = torch.Tensor(labels)\n",
    "\n",
    "# test = TensorDataset(test_data, labels)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "        test_set,\n",
    "        batch_size=1000,\n",
    "        num_workers=0,\n",
    "        shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, y_gt = evaluate(model, test_loader)\n",
    "y_pred, y_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {}\n",
    "for i in range(10000):\n",
    "    res[y_gt[i]] = y_pred[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('second_day.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Filename\", \"Label\"])\n",
    "    for key, item in res.items():\n",
    "        writer.writerow([key, item])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
